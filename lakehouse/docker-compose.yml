services:
  # === STORAGE LAYER ===
  minio:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z
    container_name: minio
    restart: unless-stopped
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_DOMAIN=${MINIO_DOMAIN}
    networks:
      iceberg_net:
        aliases:
          - warehouse.minio
    ports:
      - "0.0.0.0:9000:9000"            # S3 API
      - "0.0.0.0:9001:9001"            # Console
    volumes:
      - ./data/minio:/data
    command: ["server", "/data", "--console-address", ":9001"]

  mc:
    depends_on:
      - minio
    image: minio/mc:latest
    container_name: minio-cli
    networks:
      iceberg_net:
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
    entrypoint: |
      /bin/sh -c "
      until (/usr/bin/mc alias set minio http://minio:9000 \$$AWS_ACCESS_KEY_ID \$$AWS_SECRET_ACCESS_KEY) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc mb --ignore-existing minio/warehouse;
      /usr/bin/mc policy set public minio/warehouse;
      echo 'MinIO bucket initialized';
      exit 0
      "

  nessie-init:
    image: busybox:latest
    container_name: nessie-init
    volumes:
      - ./data/nessie:/nessie/data
    command: sh -c "chown -R 10000:10000 /nessie/data"
    user: root

  nessie:
    image: ghcr.io/projectnessie/nessie:0.99.0  # Check: curl -s localhost:19120/api/v2/config | jq .version
    container_name: nessie
    restart: unless-stopped
    depends_on:
      nessie-init:
        condition: service_completed_successfully
    networks:
      iceberg_net:
    ports:
      - "0.0.0.0:19120:19120"
    environment:
      - NESSIE_VERSION_STORE_TYPE=ROCKSDB
      - NESSIE_VERSION_STORE_PERSIST_ROCKS_DATABASE_PATH=/nessie/data
    volumes:
      - ./data/nessie:/nessie/data
      
  # === COMPUTE LAYER ===
  spark-iceberg:
    image: tabulario/spark-iceberg:3.5.5_1.8.1
    container_name: spark-iceberg
    restart: unless-stopped
    build: spark/
    networks:
      iceberg_net:
    depends_on:
      - nessie
      - minio
    volumes:
      - ./spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
    ports:
      - "0.0.0.0:8080:8080"            # Spark UI
      - "0.0.0.0:10000:10000"          # Thrift server
      - "0.0.0.0:10001:10001"          # Thrift HTTP
      - "0.0.0.0:7077:7077"            # Spark master
      - "0.0.0.0:4040:4040"            # Spark application UI
      - "0.0.0.0:15002:15002"          # Spark Connect (remote PySpark)
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        # Create required directories
        mkdir -p /tmp/spark-events
        
        # Start Spark Connect server
        /opt/spark/sbin/start-connect-server.sh \
          --packages org.apache.spark:spark-connect_2.12:3.5.5 \
          --conf spark.connect.grpc.binding.port=15002 \
          --conf spark.connect.grpc.binding.address=0.0.0.0
        
        # Keep container running
        tail -f /opt/spark/logs/*

  # === QUERY LAYER ===
  trino:
    image: trinodb/trino:479
    container_name: trino
    restart: unless-stopped
    networks:
      iceberg_net:
    depends_on:
      - nessie
      - minio
    ports:
      - "0.0.0.0:8085:8080"
    volumes:
      - ./trino/catalog:/etc/trino/catalog:ro
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}

networks:
  iceberg_net:
